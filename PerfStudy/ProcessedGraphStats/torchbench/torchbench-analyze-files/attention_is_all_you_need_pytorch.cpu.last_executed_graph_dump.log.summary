=======================================================================================================
Note: "others" is the sum of unreported ops (block*() block-ret return graph graph-others )
=======================================================================================================
Logfile (ir counts)                                                 aten::*    prim::*    fb::*    quantized::*    internal::*    caffe2::*    if       loop    call    setattr    getattr    all [others]
------------------------------------------------------------------  ---------  ---------  -------  --------------  -------------  -----------  -------  ------  ------  ---------  ---------  --------------
attention_is_all_you_need_pytorch.cpu.last_executed_graph_dump.log  564 (46%)  645 (53%)  -        -               -              -            2 ( 0%)  -       -       -          561 (46%)  1222 [11]
-------------------------------------------------------------------------------------------------------

Detailed op stats for [attention_is_all_you_need_pytorch.cpu.last_executed_graph_dump.log]
    - "prim::Loop": 0 found
    - "prim::CallMethod": 0 found
    - "prim::If": 1 distinct sources (source:line [count])
        + /home/pengwu/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1897 [2]
    - "fb::*": not found
    - "caffe2::*": not found
    - "quantized::*": not found
    - "prim::*": 7 distinct names (name [count])
        + prim::Constant (16)
        + prim::GetAttr (561)
        + prim::If (2)
        + prim::ListConstruct (62)
        + prim::ListUnpack (1)
        + prim::RaiseException (2)
        + prim::device (1)
    - "internal::*": not found
    - "aten::*": 28 distinct names (name [count])
        + aten::__and__ (1)
        + aten::add (3)
        + aten::add_ (30)
        + aten::clone (2)
        + aten::contiguous (18)
        + aten::detach (2)
        + aten::div (18)
        + aten::dropout (50)
        + aten::embedding (2)
        + aten::eq (2)
        + aten::layer_norm (32)
        + aten::linear (97)
        + aten::lt (2)
        + aten::masked_fill (18)
        + aten::matmul (36)
        + aten::mul (1)
        + aten::ne (2)
        + aten::neg (1)
        + aten::ones (1)
        + aten::relu (12)
        + aten::size (43)
        + aten::slice (4)
        + aten::softmax (18)
        + aten::to (1)
        + aten::transpose (90)
        + aten::triu (1)
        + aten::unsqueeze (4)
        + aten::view (73)
    - "prim::GetAttr": 36 distinct attr names (attr [count])
        + "0" (2)
        + "1" (2)
        + "2" (2)
        + "3" (2)
        + "4" (2)
        + "5" (2)
        + "attention" (18)
        + "bias" (56)
        + "d_k" (18)
        + "d_v" (18)
        + "decoder" (1)
        + "dropout" (50)
        + "enc_attn" (6)
        + "encoder" (1)
        + "fc" (18)
        + "layer_norm" (32)
        + "layer_stack" (2)
        + "n_head" (18)
        + "pos_ffn" (12)
        + "pos_table" (2)
        + "position_enc" (2)
        + "slf_attn" (12)
        + "src_pad_idx" (1)
        + "src_word_emb" (1)
        + "temperature" (18)
        + "training" (50)
        + "trg_pad_idx" (1)
        + "trg_word_emb" (1)
        + "trg_word_prj" (1)
        + "w_1" (12)
        + "w_2" (12)
        + "w_ks" (18)
        + "w_qs" (18)
        + "w_vs" (18)
        + "weight" (131)
        + "x_logit_scale" (1)
